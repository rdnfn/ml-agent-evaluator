"""
For licensing see accompanying LICENSE file.
Copyright (C) 2025 Apple Inc. All Rights Reserved.

Prepares the data from remote sources.

1. Downloads original remote data.
2. Merges remote data with local labels that are checked in."""

from datasets import load_dataset
from pathlib import Path
import os
import argparse
import tqdm
import json
import hashlib
import pandas as pd
import numpy as np
import ageval.datasets.rewardbench
from ageval.datasets.longfact import load_original_row_from_hashed_prompt
import shutil
import tempfile
import shlex
import subprocess
import glob


def _hash_str(input_str: str) -> str:
    return hashlib.sha256(input_str.encode("utf-8")).hexdigest()


def prepare_for_git_longfact():
    """Updates data/generated/longfact"""
    directory = Path(__file__).parent / "generated" / "longfact" / "**" / "*.*"
    all_files = glob.glob(str(directory))
    all_files.extend(glob.glob(str(directory.parent.parent / "*.*")))
    for full_path in tqdm.tqdm(all_files, desc="Reducing longfact dataset"):
        if full_path.endswith(".jsonl"):
            with open(full_path) as in_file:
                dicts = json.load(in_file)
            for row in dicts:
                content = row["messages"][0]["content"]
                row["messages"][0]["content"] = _hash_str(content.strip())
            with open(full_path, "wt") as out_file:
                json.dump(dicts, out_file, indent=4)
        elif full_path.endswith(".csv"):
            df = pd.read_csv(full_path, index_col="index")
            # text_a, text_b and original_text are generated by us.
            df["prompt"] = df["prompt"].map(lambda x: _hash_str(x.replace("\u2019", "'").replace("automa- tion", "automation").strip()))
            df.to_csv(full_path, index=True, index_label="index")


def prepare_for_use_longfact():
    """Updates data/generated/longfact"""
    directory = Path(__file__).parent / "generated" / "longfact" / "**" / "*.*"
    all_files = glob.glob(str(directory))
    all_files.extend(glob.glob(str(directory.parent.parent / "*.*")))
    for full_path in tqdm.tqdm(all_files, desc="Reducing longfact dataset"):
        if full_path.endswith(".jsonl"):
            with open(full_path) as in_file:
                dicts = json.load(in_file)
            for row in dicts:
                original_row = load_original_row_from_hashed_prompt(
                    hashed_prompts=[row["messages"][0]["content"]],
                    path=Path(
                        "third_party/long_form_factuality/longfact/longfact-objects_gpt4_01-12-2024_noduplicates"
                    ),
                )[0]
                row["messages"][0]["content"] = original_row["prompt"]
            with open(full_path, "wt") as out_file:
                json.dump(dicts, out_file, indent=4)
        elif full_path.endswith(".csv"):
            def unhash(row):
                original_row = load_original_row_from_hashed_prompt(
                    hashed_prompts=[row["prompt"]],
                    path=Path(
                        "third_party/long_form_factuality/longfact/longfact-objects_gpt4_01-12-2024_noduplicates"
                    ),
                )[0]
                row["prompt"] = original_row["prompt"]
                return row

            df = pd.read_csv(full_path, index_col="index")
            df: pd.DataFrame = df.apply(unhash, axis=1)
            df.to_csv(full_path, index=True, index_label="index")


def prepare_for_git_alpacaeval():
    """Updates data/external/alpacaeval"""
    directory = Path(__file__).parent / "external" / "alpacaeval"
    for filename in tqdm.tqdm(
        os.listdir(directory), desc="Reducing alpacaeval dataset"
    ):
        full_path = directory / filename
        if filename.endswith(".csv"):
            df = pd.read_csv(full_path, index_col="index")
            df["text_a"] = df["text_a"].map(lambda x: _hash_str(x))
            df["text_b"] = df["text_b"].map(lambda x: _hash_str(x))
            if "prompt" in df.columns:
                df["prompt"] = df["prompt"].map(lambda x: _hash_str(x))
            df.to_csv(full_path)


def prepare_for_use_alpacaeval():
    """Updates data/external/alpacaeval"""

    def get_vote(group):
        # See `third_party/icai/parsing_alpacaeval_data.ipynb`.
        vote_counts = group["preference"].value_counts()
        if vote_counts.max() == 2:  # tie
            return np.nan
        else:
            return vote_counts.idxmax()

    def unhash_row_fn(prompt_separate: bool):
        def unhash_row(row):
            if not prompt_separate:
                row["output_1"] = (
                    "Instruction:\n"
                    + row["instruction"]
                    + "\n\n\nAssistant:\n"
                    + row["output_1"]
                )
                row["output_2"] = (
                    "Instruction:\n"
                    + row["instruction"]
                    + "\n\n\nAssistant:\n"
                    + row["output_2"]
                )
            else:
                row["prompt"] = row["instruction"]
            row["output_1_hash"] = _hash_str(row["output_1"])
            row["output_2_hash"] = _hash_str(row["output_2"])
            if row["text_a"] == row["output_1_hash"]:
                row["text_a"] = row["output_1"]
                assert row["text_b"] == row["output_2_hash"]
                row["text_b"] = row["output_2"]
            else:
                assert row["text_a"] == row["output_2_hash"]
                assert row["text_b"] == row["output_1_hash"]
                row["text_a"] = row["output_2"]
                row["text_b"] = row["output_1"]
            return row

        return unhash_row

    hf_dataset = load_dataset(
        "tatsu-lab/alpaca_eval",
        "alpaca_farm_human_crossannotations",
        split="validation",
        trust_remote_code=True,
    )
    hf_dataset.set_format(type="pandas")
    hf_df: pd.DataFrame = hf_dataset[:]
    # See `third_party/icai/parsing_alpacaeval_data.ipynb`.
    hf_df = (
        hf_df.groupby(["instruction", "output_1", "output_2"])
        .apply(get_vote)
        .reset_index(name="preference")
    )
    directory = Path(__file__).parent / "external" / "alpacaeval"
    for filename in tqdm.tqdm(
        os.listdir(directory), desc="Creating alpacaeval dataset"
    ):
        full_path = directory / filename
        if filename.endswith(".csv"):
            original_df = pd.read_csv(full_path, index_col="index")
            df = original_df.join(hf_df, how="left")
            df: pd.DataFrame = df.apply(
                unhash_row_fn(prompt_separate="prompt" in original_df.columns), axis=1
            )
            df = df[original_df.columns]
            df.to_csv(full_path, index=True, index_label="index")


def prepare_for_git_truthfulqa():
    """Updates data/external/truthful_qa"""
    directory = Path(__file__).parent / "external" / "truthful_qa"
    for filename in tqdm.tqdm(
        os.listdir(directory), desc="Reducing TruthfulQA dataset"
    ):
        full_path = directory / filename
        if filename == "TruthfulQA_v0.csv":
            # Version from Huggingface.
            os.remove(full_path)
        elif filename == "TruthfulQA_v1.csv":
            # Version from repository.
            os.remove(full_path)
        elif filename.endswith(".csv"):
            df = pd.read_csv(full_path, index_col="index")
            df["prompt"] = df["prompt"].map(lambda x: _hash_str(x))
            df["text_a"] = df["text_a"].map(lambda x: _hash_str(x))
            df["text_b"] = df["text_b"].map(lambda x: _hash_str(x))
            df.to_csv(full_path)


def prepare_for_use_truthfulqa():
    """Updates data/external/alpacaeval"""
    directory = Path(__file__).parent / "external" / "truthful_qa"
    # Raw data from their repository (slightly different answers).
    with tempfile.TemporaryDirectory() as temp_dir:
        subprocess.run(
            shlex.split("git clone https://github.com/sylinrl/TruthfulQA.git"),
            cwd=temp_dir,
        )
        repo_path = os.path.join(temp_dir, "TruthfulQA")
        subprocess.run(
            shlex.split("git checkout 8b48f682acc3a71cd04e32e46e6da40ddb1a5860"),
            cwd=repo_path,
        )
        shutil.copy2(
            os.path.join(repo_path, "TruthfulQA.csv"), directory / "TruthfulQA_v1.csv"
        )
        subprocess.run(
            shlex.split("git checkout 435477c3c2323a6d1ccc7e956364d2c0899eed6a"),
            cwd=repo_path,
        )
        shutil.copy2(
            os.path.join(repo_path, "TruthfulQA.csv"), directory / "TruthfulQA_v0.csv"
        )

    def unhash_row(row):
        preferred_col = row["preferred_text"]
        incorrect_col = "text_b" if preferred_col == "text_a" else "text_a"
        row[preferred_col] = row["Best Answer"]
        
        # Handle case where hashed_IncorrectAnswers might be NaN/float
        if pd.isna(row["hashed_IncorrectAnswers"]) or not isinstance(row["hashed_IncorrectAnswers"], list):
            # If hashed_IncorrectAnswers is invalid, use first incorrect answer as fallback
            if isinstance(row["Incorrect Answers"], str):
                try:
                    incorrect_answers = eval(row["Incorrect Answers"])
                except:
                    incorrect_answers = [row["Incorrect Answers"]]
            elif pd.isna(row["Incorrect Answers"]):
                incorrect_answers = ["No answer available"]
            else:
                incorrect_answers = row["Incorrect Answers"]
            
            if isinstance(incorrect_answers, list) and len(incorrect_answers) > 0:
                row[incorrect_col] = incorrect_answers[0]
            else:
                row[incorrect_col] = "No answer available"
        else:
            row[incorrect_col] = row["Incorrect Answers"][
                list(row["hashed_IncorrectAnswers"]).index(row[incorrect_col])
            ]
        return row

    hf_df: pd.DataFrame = pd.read_csv(directory / "TruthfulQA_v1.csv")
    hf_df = hf_df[["Question", "Best Answer", "Incorrect Answers"]]
    hf_df["prompt"] = hf_df["Question"].map(lambda x: _hash_str(x))
    hf_df["hashed_Answer"] = hf_df["Best Answer"].map(lambda x: _hash_str(x))
    hf_df["Incorrect Answers"] = hf_df["Incorrect Answers"].map(
        lambda x: [answer.strip() for answer in x.split(";")]
    )
    hf_df["hashed_IncorrectAnswers"] = hf_df["Incorrect Answers"].map(
        lambda x: [_hash_str(answer) for answer in x]
    )
    for filename in tqdm.tqdm(
        os.listdir(directory), desc="Creating TruthfulQA dataset"
    ):
        full_path = directory / filename
        if filename.endswith(".csv") and filename not in (
            "TruthfulQA_v0.csv",
            "TruthfulQA_v1.csv",
        ):
            df = pd.read_csv(full_path, index_col="index")
            df = df.merge(hf_df, on="prompt", how="left")
            df = df.apply(unhash_row, axis=1)
            df = df.drop(columns=["prompt"]).rename(columns={"Question": "prompt"})
            # To preserve ordering.
            df = df[["prompt", "text_a", "text_b", "preferred_text"]]
            df.to_csv(full_path, index=True, index_label="index")


def prepare_for_git_gsm8k():
    """Updates data/external/gsm8k

    Hashes external data."""
    directory = Path(__file__).parent / "external" / "gsm8k"
    for filename in tqdm.tqdm(os.listdir(directory), desc="Reducing GSM8K dataset"):
        full_path = directory / filename
        if filename.endswith(".csv"):
            df = pd.read_csv(full_path, index_col="index")
            df["prompt"] = df["prompt"].map(lambda x: _hash_str(x))
            df.to_csv(full_path)


def prepare_for_use_gsm8k():
    """Updates data/external/gsm8k

    Replaces hashes with original prompts."""
    print("Downloading GSM8K dataset from HF...")
    hf_dataset = load_dataset(path="openai/gsm8k", name="main", split="test")
    hf_dataset.set_format(type="pandas")
    hf_df: pd.DataFrame = hf_dataset[:]
    hf_df["prompt"] = hf_df["question"].map(lambda x: _hash_str(x))
    hf_df = hf_df[["question", "prompt"]]
    directory = Path(__file__).parent / "external" / "gsm8k"
    for filename in tqdm.tqdm(os.listdir(directory), desc="Creating GSM8K dataset"):
        full_path = directory / filename
        if filename.endswith(".csv"):
            df = pd.read_csv(full_path, index_col="index")
            # Ensure prompt columns have same dtype to avoid merge errors
            df["prompt"] = df["prompt"].astype(str)
            hf_df["prompt"] = hf_df["prompt"].astype(str)
            df = df.merge(hf_df, on="prompt", how="left")
            df = df.drop(columns=["prompt"]).rename(columns={"question": "prompt"})
            # To preserve ordering.
            df = df[["prompt", "text_a", "text_b", "preferred_text"]]
            df.to_csv(full_path, index=True, index_label="index")


def prepare_for_git_rewardbench():
    """Updates data/external/rewardbench

    Removes any copy of external data."""
    shutil.rmtree(Path(__file__).parent / "external" / "rewardbench")


def prepare_for_use_rewardbench():
    """Updates data/external/rewardbench

    Completes the checked in files with remote data."""
    SUBSETS = [
        "hep-python",
        "llmbar-natural",
        "llmbar-adver-manual",
        "alpacaeval-hard",
        "math-prm",
    ]
    SUBSETS = [
        "math-prm",
        "xstest-should-respond",
        "hep-python",
        "hep-java",
        "hep-js",
        "hep-cpp",
        "hep-go",
        "hep-rust",
        "xstest-should-refuse",
        "donotanswer",
        "llmbar-adver-neighbor",
        "refusals-dangerous",
        "llmbar-natural",
        "alpacaeval-easy",
        "refusals-offensive",
        "alpacaeval-hard",
        "alpacaeval-length",
        "llmbar-adver-GPTInst",
        "llmbar-adver-GPTOut",
        "llmbar-adver-manual",
        "mt-bench-med",
        "mt-bench-hard",
        "mt-bench-easy",
    ]
    folder = Path(__file__).parent / "external" / "rewardbench"
    os.makedirs(folder, exist_ok=True)
    for subset in tqdm.tqdm(SUBSETS, desc="Creating RewardBench dataset"):
        df = ageval.datasets.rewardbench.load(subset=subset)
        path = folder / f"{subset}.csv"
        df.to_csv(path, index=True, index_label="index")


def prepare_for_git_code_apps():
    """Updates data/external/code_apps

    Removes any copy of external data."""
    directory = Path(__file__).parent / "external" / "code_apps"
    for filename in tqdm.tqdm(os.listdir(directory), desc="Reducing APPS dataset"):
        full_path = directory / filename
        if filename.endswith(".json"):
            with open(full_path) as in_file:
                loaded_json: dict[str, dict] = json.load(in_file)
            for row in loaded_json.values():
                del row["question"]
                del row["solution"]
            with open(full_path, "wt") as out_file:
                json.dump(loaded_json, out_file, indent=4)
        elif filename.endswith(".csv"):
            df = pd.read_csv(full_path, index_col="index")
            df = df.drop(["prompt"], axis=1)
            df.to_csv(full_path)


def prepare_for_use_code_apps():
    """Updates data/external/code_apps

    Completes the checked in files with data from huggingface."""
    hf_dataset = load_dataset(path="codeparrot/apps", name="competition", split="test")
    hf_dataset.set_format(type="pandas")
    hf_df = hf_dataset[:]
    directory = Path(__file__).parent / "external" / "code_apps"
    for filename in tqdm.tqdm(os.listdir(directory), desc="Creating APPS dataset"):
        full_path = directory / filename
        if filename.endswith(".json"):
            with open(full_path) as in_file:
                loaded_json: dict[str, dict] = json.load(in_file)
            for problem_id, row in loaded_json.items():
                hf_row = hf_df[hf_df["problem_id"] == int(problem_id)]
                row["question"] = hf_row["question"].iloc[0]
                row["solution"] = eval(hf_row["solutions"].iloc[0])[0]
            with open(full_path, "wt") as out_file:
                json.dump(loaded_json, out_file, indent=4)
        elif filename.endswith(".csv"):
            df = pd.read_csv(full_path)
            df = df.set_index("index")
            # Drop existing prompt column if it exists to avoid join conflicts
            if "prompt" in df.columns:
                df = df.drop("prompt", axis=1)
            hf_df_prompts = (
                hf_df[["problem_id", "question"]]
                .rename(columns={"problem_id": "index", "question": "prompt"})
                .set_index("index")
            )
            df = df.join(hf_df_prompts, how="left")
            # To preserve ordering.
            df = df[["text_a", "text_b", "prompt", "preferred_text"]]
            df.to_csv(full_path)


def prepare_for_use_all():
    prepare_for_use_code_apps()
    prepare_for_use_rewardbench()
    prepare_for_use_gsm8k()
    prepare_for_use_truthfulqa()
    prepare_for_use_alpacaeval()
    prepare_for_use_longfact()


def prepare_for_git_all():
    prepare_for_git_code_apps()
    prepare_for_git_rewardbench()
    prepare_for_git_gsm8k()
    prepare_for_git_truthfulqa()
    prepare_for_git_alpacaeval()
    prepare_for_git_longfact()


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--mode",
        "-m",
        choices=["use", "git"],
        help="Prepare for local use or for remote git.",
    )
    args = parser.parse_args()

    if args.mode == "use":
        prepare_for_use_all()
    elif args.mode == "git":
        prepare_for_git_all()
    else:
        raise ValueError()


if __name__ == "__main__":
    main()
